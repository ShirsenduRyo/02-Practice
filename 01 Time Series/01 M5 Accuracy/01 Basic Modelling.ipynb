{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current folder is: d:\\01 Work\\01 Coding\\02-Practice\\01 Time Series\\01 M5 Accuracy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "current_folder = os.getcwd()\n",
    "\n",
    "print(f\"The current folder is: {current_folder}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgc\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = '../01 M5 Accuracy/01 Data'\n",
    "calender = pd.read_csv(f'{INPUT_DIR}/calendar.csv')\n",
    "sales = pd.read_csv(f'{INPUT_DIR}/sales_train_validation.csv')\n",
    "prices = pd.read_csv(f'{INPUT_DIR}/sell_prices.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data Added\n",
    "for d in range(1942,1970):\n",
    "    col = 'd_' + str(d)\n",
    "    sales[col] = 0\n",
    "    sales[col] = sales[col].astype(np.int16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Downcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def downcast(df):\n",
    "    cols = df.dtypes.index.tolist()\n",
    "    types = df.dtypes.values.tolist()\n",
    "    for i,t in enumerate(types):\n",
    "        if 'int' in str(t):\n",
    "            if df[cols[i]].min() > np.iinfo(np.int8).min and df[cols[i]].max() < np.iinfo(np.int8).max:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.int8)\n",
    "            elif df[cols[i]].min() > np.iinfo(np.int16).min and df[cols[i]].max() < np.iinfo(np.int16).max:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.int16)\n",
    "            elif df[cols[i]].min() > np.iinfo(np.int32).min and df[cols[i]].max() < np.iinfo(np.int32).max:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.int32)\n",
    "            else:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.int64)\n",
    "        elif 'float' in str(t):\n",
    "            if df[cols[i]].min() > np.finfo(np.float16).min and df[cols[i]].max() < np.finfo(np.float16).max:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.float16)\n",
    "            elif df[cols[i]].min() > np.finfo(np.float32).min and df[cols[i]].max() < np.finfo(np.float32).max:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.float32)\n",
    "            else:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.float64)\n",
    "\n",
    "        elif 'date' in str(t):\n",
    "                df[cols[i]] = pd.to_datetime(df[cols[i]], format='%Y-%m-%d')\n",
    "        else:\n",
    "                df[cols[i]] = df[cols[i]].astype('category')\n",
    "    return df  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sales = downcast(sales)\n",
    "prices = downcast(prices)\n",
    "calender = downcast(calender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data Melting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.melt(sales, id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name='d', value_name='sold').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, calender, on='d', how='left')\n",
    "df = pd.merge(df, prices, on=['store_id','item_id','wm_yr_wk'], how='left') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Store Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d_id = dict(zip(df.id.cat.codes, df.id))\n",
    "d_item_id = dict(zip(df.item_id.cat.codes, df.item_id))\n",
    "d_dept_id = dict(zip(df.dept_id.cat.codes, df.dept_id))\n",
    "d_cat_id = dict(zip(df.cat_id.cat.codes, df.cat_id))\n",
    "d_store_id = dict(zip(df.store_id.cat.codes, df.store_id))\n",
    "d_state_id = dict(zip(df.state_id.cat.codes, df.state_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#2\n",
    "df.d = df['d'].apply(lambda x: x.split('_')[1]).astype(np.int16)\n",
    "cols = df.dtypes.index.tolist()\n",
    "types = df.dtypes.values.tolist()\n",
    "for i,type in enumerate(types):\n",
    "    if type.name == 'category':\n",
    "        df[cols[i]] = df[cols[i]].cat.codes\n",
    "        \n",
    "#3\n",
    "df.drop('date',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lags = [1,2,3,6,12,24,36]\n",
    "for lag in lags:\n",
    "    df['sold_lag_'+str(lag)] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],as_index=False)['sold'].shift(lag).astype(np.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Mean Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['iteam_sold_avg'] = df.groupby('item_id')['sold'].transform('mean').astype(np.float16)\n",
    "df['state_sold_avg'] = df.groupby('state_id')['sold'].transform('mean').astype(np.float16)\n",
    "df['store_sold_avg'] = df.groupby('store_id')['sold'].transform('mean').astype(np.float16)\n",
    "df['cat_sold_avg'] = df.groupby('cat_id')['sold'].transform('mean').astype(np.float16)\n",
    "df['dept_sold_avg'] = df.groupby('dept_id')['sold'].transform('mean').astype(np.float16)\n",
    "df['cat_dept_sold_avg'] = df.groupby(['cat_id','dept_id'])['sold'].transform('mean').astype(np.float16)\n",
    "df['store_item_sold_avg'] = df.groupby(['store_id','item_id'])['sold'].transform('mean').astype(np.float16)\n",
    "df['cat_item_sold_avg'] = df.groupby(['cat_id','item_id'])['sold'].transform('mean').astype(np.float16)\n",
    "df['dept_item_sold_avg'] = df.groupby(['dept_id','item_id'])['sold'].transform('mean').astype(np.float16)\n",
    "df['state_store_sold_avg'] = df.groupby(['state_id','store_id'])['sold'].transform('mean').astype(np.float16)\n",
    "df['state_store_cat_sold_avg'] = df.groupby(['state_id','store_id','cat_id'])['sold'].transform('mean').astype(np.float16)\n",
    "df['store_cat_dept_sold_avg'] = df.groupby(['store_id','cat_id','dept_id'])['sold'].transform('mean').astype(np.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Rolling Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rolling_sold_mean'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sold'].transform(lambda x: x.rolling(window=7).mean()).astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['expanding_sold_mean'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sold'].transform(lambda x: x.expanding(2).mean()).astype(np.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['daily_avg_sold'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id','d'])['sold'].transform('mean').astype(np.float16)\n",
    "df['avg_sold'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sold'].transform('mean').astype(np.float16)\n",
    "df['selling_trend'] = (df['daily_avg_sold'] - df['avg_sold']).astype(np.float16)\n",
    "df.drop(['daily_avg_sold','avg_sold'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Save The Intermediate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02 Internediate Data\n",
    "#Lag Introduced till Day 36\n",
    "df = df[df['d']>=36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 58113940 entries, 1067150 to 59181089\n",
      "Data columns (total 43 columns):\n",
      " #   Column                    Dtype  \n",
      "---  ------                    -----  \n",
      " 0   id                        int16  \n",
      " 1   item_id                   int16  \n",
      " 2   dept_id                   int8   \n",
      " 3   cat_id                    int8   \n",
      " 4   store_id                  int8   \n",
      " 5   state_id                  int8   \n",
      " 6   d                         int16  \n",
      " 7   sold                      int16  \n",
      " 8   wm_yr_wk                  int16  \n",
      " 9   weekday                   int8   \n",
      " 10  wday                      int8   \n",
      " 11  month                     int8   \n",
      " 12  year                      int16  \n",
      " 13  event_name_1              int8   \n",
      " 14  event_type_1              int8   \n",
      " 15  event_name_2              int8   \n",
      " 16  event_type_2              int8   \n",
      " 17  snap_CA                   int8   \n",
      " 18  snap_TX                   int8   \n",
      " 19  snap_WI                   int8   \n",
      " 20  sell_price                float16\n",
      " 21  sold_lag_1                float16\n",
      " 22  sold_lag_2                float16\n",
      " 23  sold_lag_3                float16\n",
      " 24  sold_lag_6                float16\n",
      " 25  sold_lag_12               float16\n",
      " 26  sold_lag_24               float16\n",
      " 27  sold_lag_36               float16\n",
      " 28  iteam_sold_avg            float16\n",
      " 29  state_sold_avg            float16\n",
      " 30  store_sold_avg            float16\n",
      " 31  cat_sold_avg              float16\n",
      " 32  dept_sold_avg             float16\n",
      " 33  cat_dept_sold_avg         float16\n",
      " 34  store_item_sold_avg       float16\n",
      " 35  cat_item_sold_avg         float16\n",
      " 36  dept_item_sold_avg        float16\n",
      " 37  state_store_sold_avg      float16\n",
      " 38  state_store_cat_sold_avg  float16\n",
      " 39  store_cat_dept_sold_avg   float16\n",
      " 40  rolling_sold_mean         float16\n",
      " 41  expanding_sold_mean       float16\n",
      " 42  selling_trend             float16\n",
      "dtypes: float16(23), int16(6), int8(14)\n",
      "memory usage: 4.3 GB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_pickle('02 Internediate Data/data.pkl')\n",
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
