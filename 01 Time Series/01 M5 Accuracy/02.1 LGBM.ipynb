{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('64bit', 'WindowsPE')\n",
      "c:\\Users\\sport\\miniconda3\\python.exe\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import platform; print(platform.architecture())\"\n",
    "!python -c \"import sys; print(sys.executable)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('02 Internediate Data/data.pkl')\n",
    "valid = data[(data['d']>=1914) & (data['d']<1942)][['id','d','sold']]\n",
    "test = data[data['d']>=1942][['id','d','sold']]\n",
    "eval_preds = test['sold']\n",
    "valid_preds = valid['sold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((853720, 3), (853720, 3))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = '../01 M5 Accuracy/01 Data'\n",
    "calender = pd.read_csv(f'{INPUT_DIR}/calendar.csv')\n",
    "sales = pd.read_csv(f'{INPUT_DIR}/sales_train_validation.csv')\n",
    "prices = pd.read_csv(f'{INPUT_DIR}/sell_prices.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales['store_id'] = sales['store_id'].astype('category')\n",
    "data['store_id'] = data['store_id'].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the store ids\n",
    "stores = sales.store_id.cat.codes.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_store_id = dict(zip(data.store_id.cat.codes, data.store_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                 0\n",
       "item_id                            0\n",
       "dept_id                            0\n",
       "cat_id                             0\n",
       "store_id                           0\n",
       "state_id                           0\n",
       "d                                  0\n",
       "sold                               0\n",
       "wm_yr_wk                           0\n",
       "weekday                            0\n",
       "wday                               0\n",
       "month                              0\n",
       "year                               0\n",
       "event_name_1                       0\n",
       "event_type_1                       0\n",
       "event_name_2                       0\n",
       "event_type_2                       0\n",
       "snap_CA                            0\n",
       "snap_TX                            0\n",
       "snap_WI                            0\n",
       "sell_price                  11661069\n",
       "sold_lag_1                         0\n",
       "sold_lag_2                         0\n",
       "sold_lag_3                         0\n",
       "sold_lag_6                         0\n",
       "sold_lag_12                        0\n",
       "sold_lag_24                        0\n",
       "sold_lag_36                    30490\n",
       "iteam_sold_avg                     0\n",
       "state_sold_avg                     0\n",
       "store_sold_avg                     0\n",
       "cat_sold_avg                       0\n",
       "dept_sold_avg                      0\n",
       "cat_dept_sold_avg                  0\n",
       "store_item_sold_avg                0\n",
       "cat_item_sold_avg                  0\n",
       "dept_item_sold_avg                 0\n",
       "state_store_sold_avg               0\n",
       "state_store_cat_sold_avg           0\n",
       "store_cat_dept_sold_avg            0\n",
       "rolling_sold_mean                  0\n",
       "expanding_sold_mean                0\n",
       "selling_trend                      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5726022, 42)\n",
      "(85372, 42)\n",
      "*****Prediction for Store: 0*****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sport\\miniconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] The system cannot find the file specified\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"c:\\Users\\sport\\miniconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sport\\miniconda3\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sport\\miniconda3\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"c:\\Users\\sport\\miniconda3\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.304308 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3787\n",
      "[LightGBM] [Info] Number of data points in the train set: 5726022, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 1.324972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sport\\AppData\\Local\\Temp\\ipykernel_7916\\4120228526.py:27: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[0.00818941 0.00818941 0.00818941 ... 0.92129258 1.00117114 4.93884095]' has dtype incompatible with int16, please explicitly cast to a compatible dtype first.\n",
      "  valid_preds[X_valid.index] = model.predict(X_valid)\n",
      "C:\\Users\\sport\\AppData\\Local\\Temp\\ipykernel_7916\\4120228526.py:28: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[0.00818941 0.00818941 0.00818941 ... 0.03031463 0.00818941 0.00818941]' has dtype incompatible with int16, please explicitly cast to a compatible dtype first.\n",
      "  eval_preds[X_test.index] = model.predict(X_test)\n",
      " 10%|█         | 1/10 [00:14<02:09, 14.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5726022, 42)\n",
      "(85372, 42)\n",
      "*****Prediction for Store: 1*****\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.274342 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3607\n",
      "[LightGBM] [Info] Number of data points in the train set: 5726022, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 0.978204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:28<01:55, 14.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5726022, 42)\n",
      "(85372, 42)\n",
      "*****Prediction for Store: 2*****\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.156595 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4008\n",
      "[LightGBM] [Info] Number of data points in the train set: 5726022, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 1.927989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:43<01:41, 14.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5726022, 42)\n",
      "(85372, 42)\n",
      "*****Prediction for Store: 3*****\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.303653 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3422\n",
      "[LightGBM] [Info] Number of data points in the train set: 5726022, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 0.707667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:57<01:26, 14.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5726022, 42)\n",
      "(85372, 42)\n",
      "*****Prediction for Store: 4*****\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.231047 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3740\n",
      "[LightGBM] [Info] Number of data points in the train set: 5726022, number of used features: 42\n",
      "[LightGBM] [Info] Start training from score 0.963681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [01:12<01:12, 14.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5726022, 42)\n",
      "(85372, 42)\n",
      "*****Prediction for Store: 5*****\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.266932 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3899\n",
      "[LightGBM] [Info] Number of data points in the train set: 5726022, number of used features: 42\n",
      "[LightGBM] [Info] Start training from score 1.241402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [01:26<00:57, 14.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5726022, 42)\n",
      "(85372, 42)\n",
      "*****Prediction for Store: 6*****\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.279697 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3839\n",
      "[LightGBM] [Info] Number of data points in the train set: 5726022, number of used features: 42\n",
      "[LightGBM] [Info] Start training from score 1.049304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [01:40<00:43, 14.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5726022, 42)\n",
      "(85372, 42)\n",
      "*****Prediction for Store: 7*****\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.237304 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3555\n",
      "[LightGBM] [Info] Number of data points in the train set: 5726022, number of used features: 42\n",
      "[LightGBM] [Info] Start training from score 0.889008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [01:55<00:29, 14.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5726022, 42)\n",
      "(85372, 42)\n",
      "*****Prediction for Store: 8*****\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.224389 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3783\n",
      "[LightGBM] [Info] Number of data points in the train set: 5726022, number of used features: 42\n",
      "[LightGBM] [Info] Start training from score 1.131519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [02:10<00:14, 14.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5726022, 42)\n",
      "(85372, 42)\n",
      "*****Prediction for Store: 9*****\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.240838 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3826\n",
      "[LightGBM] [Info] Number of data points in the train set: 5726022, number of used features: 42\n",
      "[LightGBM] [Info] Start training from score 1.101072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:24<00:00, 14.49s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for store in tqdm(stores):\n",
    "    df = data[data['store_id']==store]\n",
    "    \n",
    "    #Split the data\n",
    "    X_train, y_train = df[df['d']<1914].drop('sold',axis=1), df[df['d']<1914]['sold']\n",
    "    X_valid, y_valid = df[(df['d']>=1914) & (df['d']<1942)].drop('sold',axis=1), df[(df['d']>=1914) & (df['d']<1942)]['sold']\n",
    "    X_test = df[df['d']>=1942].drop('sold',axis=1)\n",
    "    print(X_train.shape)\n",
    "    print(X_valid.shape)\n",
    "\n",
    "    #Train and validate\n",
    "    model = LGBMRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.3,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        max_depth=6,\n",
    "        num_leaves=5,\n",
    "        min_child_weight=15\n",
    "    )\n",
    "    print('*****Prediction for Store: {}*****'.format(d_store_id[store]))\n",
    "    model.fit(X_train, y_train, eval_set=[(X_train,y_train),(X_valid,y_valid)],\n",
    "             eval_metric='rmse')\n",
    "    \n",
    "    \n",
    "    \n",
    "    valid_preds[X_valid.index] = model.predict(X_valid)\n",
    "    eval_preds[X_test.index] = model.predict(X_test)\n",
    "    filename = 'model'+str(d_store_id[store])+'.pkl'\n",
    "\n",
    "    joblib.dump(model, filename)\n",
    "    del model, X_train, y_train, X_valid, y_valid\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_importance_df = pd.DataFrame()\n",
    "# features = [f for f in data.columns if f != 'sold']\n",
    "# for filename in tqdm(os.listdir('/kaggle/working/'):\n",
    "#     if 'model' in filename:\n",
    "#         # load model\n",
    "#         model = joblib.load(filename)\n",
    "#         store_importance_df = pd.DataFrame()\n",
    "#         store_importance_df[\"feature\"] = features\n",
    "#         store_importance_df[\"importance\"] = model.feature_importances_\n",
    "#         store_importance_df[\"store\"] = filename[5:9]\n",
    "#         feature_importance_df = pd.concat([feature_importance_df, store_importance_df], axis=0)\n",
    "    \n",
    "# def display_importances(feature_importance_df_):\n",
    "#     cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:20].index\n",
    "#     best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n",
    "#     plt.figure(figsize=(8, 10))\n",
    "#     sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "#     plt.title('LightGBM Features (averaged over store predictions)')\n",
    "#     plt.tight_layout()\n",
    "    \n",
    "# display_importances(feature_importance_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
